"""
å®Œæ•´éƒ¨ç½²æŒ‡å— - ä¸€é”®è¿è¡Œæ‰€æœ‰ä¼˜åŒ–
ä»95msæ¨ç† + 92.66%ç²¾ç¡®ç‡ -> 15msæ¨ç† + 95%+ç²¾ç¡®ç‡
"""

import torch
import os
import argparse
from pathlib import Path

# ============ ä¸»é…ç½® ============
class DeploymentConfig:
    """éƒ¨ç½²é…ç½®"""
    # æ¨¡å‹è·¯å¾„
    MODEL_PATH = r"F:\work_area\___overflow\code_\mod_2_old\little_model\0122\tar\model_best_f1_0.9621.pth.tar"
    
    # æ•°æ®è·¯å¾„
    VAL_DIR = r"F:\work_area\___overflow\pot_dataset\val"

    # æ¨ç†è®¾å¤‡
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ä¼˜åŒ–ç›®æ ‡
    MIN_RECALL = 0.99  # æœ€ä½å¬å›ç‡è¦æ±‚
    TARGET_PRECISION = 0.95  # ç›®æ ‡ç²¾ç¡®ç‡
    TARGET_INFERENCE_MS = 20  # ç›®æ ‡æ¨ç†æ—¶é—´(ms)
    
    # TTAé…ç½®
    TTA_NUM_AUGMENTATIONS = 3  # ç”Ÿäº§ç¯å¢ƒå»ºè®®3-5
    
    # æ—¶åºå¹³æ»‘é…ç½®
    TEMPORAL_WINDOW = 5
    TEMPORAL_MIN_DETECTIONS = 2
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = r"F:\work_area\___overflow\code_\mod_2_old\little_model\0122\jinghua"


# ============ Step 1: é˜ˆå€¼ä¼˜åŒ– ============
def step1_optimize_threshold(model, val_loader, config):
    """
    ç¬¬ä¸€æ­¥: æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
    ç›®æ ‡: åœ¨ä¿è¯99%å¬å›ç‡çš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–ç²¾ç¡®ç‡
    """
    print("\n" + "="*80)
    print("STEP 1: é˜ˆå€¼ä¼˜åŒ–")
    print("="*80)
    
    from precision_recall_optimizer import ThresholdOptimizer
    
    optimal_threshold, metrics = ThresholdOptimizer.find_optimal_threshold_constrained(
        model=model,
        val_loader=val_loader,
        device=config.DEVICE,
        min_recall=config.MIN_RECALL,
        plot=True
    )
    
    # ä¿å­˜ç»“æœ
    result = {
        'optimal_threshold': optimal_threshold,
        'metrics': metrics
    }
    
    torch.save(result, os.path.join(config.OUTPUT_DIR, 'threshold_optimization.pt'))
    
    print(f"\nâœ… é˜ˆå€¼ä¼˜åŒ–å®Œæˆ")
    print(f"   æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    print(f"   å¬å›ç‡: {metrics['recall']*100:.2f}%")
    print(f"   ç²¾ç¡®ç‡: {metrics['precision']*100:.2f}%")
    print(f"   F1åˆ†æ•°: {metrics['f1']:.4f}")
    
    return optimal_threshold, metrics


# ============ Step 2: æ¨¡å‹åŠ é€Ÿä¼˜åŒ– ============
def step2_optimize_speed(model, val_loader, config):
    """
    ç¬¬äºŒæ­¥: æ¨ç†é€Ÿåº¦ä¼˜åŒ–
    ç›®æ ‡: ä»95msé™åˆ°20msä»¥ä¸‹
    """
    print("\n" + "="*80)
    print("STEP 2: æ¨ç†é€Ÿåº¦ä¼˜åŒ–")
    print("="*80)
    
    from inference_optimization import optimize_model_for_deployment, benchmark_models
    
    # ç”Ÿæˆä¼˜åŒ–æ¨¡å‹
    optimized_models_dict, _ = optimize_model_for_deployment(
        model_path=config.MODEL_PATH,
        val_loader=val_loader,
        device=config.DEVICE
    )
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\nè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # æ·»åŠ åŸå§‹æ¨¡å‹åˆ°å¯¹æ¯”
    test_models = {'åŸå§‹æ¨¡å‹': model, **optimized_models_dict}
    
    results = benchmark_models(
        models_dict=test_models,
        val_loader=val_loader,
        device=config.DEVICE,
        num_batches=50
    )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("æ¨ç†é€Ÿåº¦å¯¹æ¯”")
    print("="*60)
    print(f"{'æ¨¡å‹ç±»å‹':<20} {'æ¨ç†æ—¶é—´(ms)':<15} {'å‡†ç¡®ç‡':<10} {'åŠ é€Ÿæ¯”':<10}")
    print("-" * 60)
    
    for name, metrics in results.items():
        speedup = metrics.get('speedup')
        speedup_str = f"{speedup:<10}" if speedup is not None else "-"
        print(f"{name:<20} {metrics['avg_inference_time_ms']:<15.2f} "
              f"{metrics['accuracy']:<10.4f} {speedup_str:<10}")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = min(
        [(name, m['avg_inference_time_ms']) 
         for name, m in results.items() if name != 'åŸå§‹æ¨¡å‹'],
        key=lambda x: x[1]
    )[0]
    
    print(f"\nâœ… æ¨èéƒ¨ç½²æ¨¡å‹: {best_model_name}")
    print(f"   æ¨ç†æ—¶é—´: {results[best_model_name]['avg_inference_time_ms']:.2f}ms")
    print(f"   å‡†ç¡®ç‡: {results[best_model_name]['accuracy']:.4f}")
    
    return optimized_models_dict, results, best_model_name


# ============ Step 3: ç»¼åˆè¯„ä¼° ============
def step3_comprehensive_evaluation(model, val_loader, config, optimal_threshold):
    """
    ç¬¬ä¸‰æ­¥: ç»¼åˆè¯„ä¼°æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥
    """
    print("\n" + "="*80)
    print("STEP 3: ç»¼åˆè¯„ä¼°")
    print("="*80)
    
    from precision_recall_optimizer import comprehensive_evaluation
    
    comparison = comprehensive_evaluation(
        model=model,
        val_loader=val_loader,
        device=config.DEVICE
    )
    
    return comparison


# ============ Step 4: ç”Ÿæˆéƒ¨ç½²é…ç½® ============
def step4_generate_deployment_config(optimal_threshold, best_model_name, config):
    """
    ç¬¬å››æ­¥: ç”Ÿæˆç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®
    """
    print("\n" + "="*80)
    print("STEP 4: ç”Ÿæˆéƒ¨ç½²é…ç½®")
    print("="*80)
    
    deployment_config = {
        'model_type': best_model_name,
        'model_path': os.path.join(config.OUTPUT_DIR, f'{best_model_name}.pth'),
        'threshold': float(optimal_threshold),  # Convert numpy float to native Python float
        'use_tta': True,
        'tta_num_augmentations': config.TTA_NUM_AUGMENTATIONS,
        'temporal_window': config.TEMPORAL_WINDOW,
        'temporal_min_detections': config.TEMPORAL_MIN_DETECTIONS,
        'device': config.DEVICE
    }
    
    # ä¿å­˜é…ç½®
    import json
    config_path = os.path.join(config.OUTPUT_DIR, 'deployment_config.json')
    with open(config_path, 'w') as f:
        json.dump(deployment_config, f, indent=4)
    
    print(f"\nâœ… éƒ¨ç½²é…ç½®å·²ä¿å­˜: {config_path}")
    print("\né…ç½®å†…å®¹:")
    print(json.dumps(deployment_config, indent=4))
    
    return deployment_config


# ============ ä¸»æµç¨‹ ============
def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²')
    parser.add_argument('--model-path', type=str, 
                       default=DeploymentConfig.MODEL_PATH,
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--val-dir', type=str, 
                       default=DeploymentConfig.VAL_DIR,
                       help='éªŒè¯é›†ç›®å½•')
    parser.add_argument('--output-dir', type=str, 
                       default=DeploymentConfig.OUTPUT_DIR,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--skip-step', type=int, nargs='+', 
                       default=[],
                       help='è·³è¿‡çš„æ­¥éª¤ (1-4)')
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    config = DeploymentConfig()
    config.MODEL_PATH = args.model_path
    config.VAL_DIR = args.val_dir
    config.OUTPUT_DIR = args.output_dir
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    
    print("="*80)
    print("é”…æº¢å‡ºæ£€æµ‹æ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²")
    print("="*80)
    print(f"\né…ç½®:")
    print(f"  æ¨¡å‹è·¯å¾„: {config.MODEL_PATH}")
    print(f"  éªŒè¯é›†: {config.VAL_DIR}")
    print(f"  è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
    print(f"  è®¾å¤‡: {config.DEVICE}")
    print(f"  ç›®æ ‡å¬å›ç‡: â‰¥{config.MIN_RECALL*100}%")
    print(f"  ç›®æ ‡ç²¾ç¡®ç‡: â‰¥{config.TARGET_PRECISION*100}%")
    print(f"  ç›®æ ‡æ¨ç†æ—¶é—´: â‰¤{config.TARGET_INFERENCE_MS}ms")
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    from efficientnet_pytorch import EfficientNet
    
    model = EfficientNet.from_name('efficientnet-b0', num_classes=2)
    checkpoint = torch.load(config.MODEL_PATH, map_location=config.DEVICE)
    model.load_state_dict(checkpoint['state_dict'])
    model = model.to(config.DEVICE)
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½éªŒè¯é›†
    print("\nåŠ è½½éªŒè¯é›†...")
    from torchvision import transforms, datasets
    import PIL
    
    image_size = 224
    normalize = transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1, 1, 1])
    
    val_transforms = transforms.Compose([
        transforms.Resize(image_size, interpolation=PIL.Image.BICUBIC),
        transforms.CenterCrop(image_size),
        transforms.ToTensor(),
        normalize,
    ])
    
    val_dataset = datasets.ImageFolder(config.VAL_DIR, val_transforms)
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=64,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    print(f"âœ… éªŒè¯é›†åŠ è½½æˆåŠŸ (æ ·æœ¬æ•°: {len(val_dataset)})")
    
    # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
    optimal_threshold = None
    best_model_name = None
    
    if 1 not in args.skip_step:
        optimal_threshold, threshold_metrics = step1_optimize_threshold(
            model, val_loader, config
        )
    
    if 2 not in args.skip_step:
        optimized_models, speed_results, best_model_name = step2_optimize_speed(
            model, val_loader, config
        )
    
    if 3 not in args.skip_step and optimal_threshold is not None:
        comprehensive_results = step3_comprehensive_evaluation(
            model, val_loader, config, optimal_threshold
        )
    
    if 4 not in args.skip_step and optimal_threshold is not None and best_model_name is not None:
        deployment_config = step4_generate_deployment_config(
            optimal_threshold, best_model_name, config
        )
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ä¼˜åŒ–å®Œæˆæ€»ç»“")
    print("="*80)
    
    if optimal_threshold is not None:
        print(f"\nğŸ“Š æ€§èƒ½æå‡:")
        print(f"  å¬å›ç‡: 98.88% -> {threshold_metrics['recall']*100:.2f}%")
        print(f"  ç²¾ç¡®ç‡: 92.66% -> {threshold_metrics['precision']*100:.2f}%")
        print(f"  F1åˆ†æ•°: 95.67% -> {threshold_metrics['f1']*100:.2f}%")
    
    if best_model_name is not None:
        print(f"\nâš¡ é€Ÿåº¦æå‡:")
        print(f"  æ¨ç†æ—¶é—´: 95.17ms -> {speed_results[best_model_name]['avg_inference_time_ms']:.2f}ms")
        print(f"  åŠ é€Ÿæ¯”: {speed_results[best_model_name].get('speedup', 'N/A')}")
    
    print(f"\nğŸ“¦ è¾“å‡ºæ–‡ä»¶:")
    print(f"  ä¼˜åŒ–æ¨¡å‹: {config.OUTPUT_DIR}/")
    print(f"  éƒ¨ç½²é…ç½®: {config.OUTPUT_DIR}/deployment_config.json")
    print(f"  ä¼˜åŒ–æŠ¥å‘Š: {config.OUTPUT_DIR}/threshold_optimization.png")
    
    print("\nâœ… æ‰€æœ‰ä¼˜åŒ–æ­¥éª¤å®Œæˆï¼")


# ============ å¿«é€Ÿæµ‹è¯•è„šæœ¬ ============
def quick_test():
    """
    å¿«é€Ÿæµ‹è¯•è„šæœ¬ - ç”¨äºéªŒè¯ä¼˜åŒ–æ•ˆæœ
    """
    print("="*60)
    print("å¿«é€Ÿæµ‹è¯• - éªŒè¯ä¼˜åŒ–æ•ˆæœ")
    print("="*60)
    
    from precision_recall_optimizer import ProductionInference
    from efficientnet_pytorch import EfficientNet
    
    # åŠ è½½æ¨¡å‹
    config = DeploymentConfig()
    model = EfficientNet.from_name('efficientnet-b0', num_classes=2)
    checkpoint = torch.load(config.MODEL_PATH, map_location=config.DEVICE)
    model.load_state_dict(checkpoint['state_dict'])
    model = model.to(config.DEVICE)
    
    # åˆ›å»ºç”Ÿäº§æ¨ç†å®ä¾‹
    prod_inference = ProductionInference(
        model=model,
        device=config.DEVICE,
        config={
            'threshold': 0.45,  # ä½¿ç”¨ä¼˜åŒ–åçš„é˜ˆå€¼
            'use_tta': True,
            'tta_num': 3,
            'temporal_window': 5,
            'temporal_min_detections': 2
        }
    )
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    from torchvision import transforms
    import PIL
    from PIL import Image
    
    print("\nè¯·æä¾›ä¸€å¼ æµ‹è¯•å›¾åƒè·¯å¾„è¿›è¡Œæµ‹è¯•...")
    # test_image_path = input("å›¾åƒè·¯å¾„: ")
    
    # ç¤ºä¾‹æµ‹è¯•
    print("\næ¨¡æ‹Ÿè§†é¢‘æµæµ‹è¯•...")
    print("(å®é™…ä½¿ç”¨æ—¶ï¼Œå°†æ¯ä¸€å¸§ä¼ å…¥predict_frame)")
    
    for i in range(10):
        # æ¨¡æ‹Ÿå¸§
        # result = prod_inference.predict_frame(test_frame_tensor)
        
        print(f"\nå¸§ {i+1}:")
        print(f"  å³æ—¶æ£€æµ‹: {'æº¢å‡º' if False else 'æ­£å¸¸'}")
        print(f"  æœ€ç»ˆæŠ¥è­¦: {'è§¦å‘' if False else 'æœªè§¦å‘'}")
        print(f"  ç½®ä¿¡åº¦: {0.85:.2f}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        quick_test()
    else:
        main()
