"""
ä¸€é”®ä¼˜åŒ–è„šæœ¬ - è¾“å‡ºæ ‡å‡†.pthæƒé‡æ–‡ä»¶
è¿è¡Œåå¾—åˆ°:
  1. model_optimized.pth - åŸºç¡€ä¼˜åŒ–ç‰ˆæœ¬
  2. model_fp16.pth - FP16ä¼˜åŒ–ç‰ˆæœ¬ (GPUæ¨è)
  3. threshold_optimization.pt - æœ€ä¼˜é˜ˆå€¼é…ç½®
  4. deployment_config.json - éƒ¨ç½²é…ç½®æ–‡ä»¶

ä½¿ç”¨æ–¹å¼:
  python simple_usage_script.py
"""

import torch
import os
from pathlib import Path

# ==================== é…ç½®åŒº ====================
# ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„å³å¯
MODEL_PATH = r"F:\work_area\___overflow\code_\mod_2_old\little_model\0122\tar\model_best_f1_0.9621.pth.tar"
VAL_DIR = r"F:\work_area\___overflow\pot_dataset\val"
OUTPUT_DIR = r"F:\work_area\___overflow\code_\mod_2_old\little_model\0122\jinghua"

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
MIN_RECALL = 0.99  # æœ€ä½å¬å›ç‡è¦æ±‚
# ================================================

def main():
    print("="*80)
    print("é”…æº¢å‡ºæ£€æµ‹æ¨¡å‹ - ä¸€é”®ä¼˜åŒ–")
    print("="*80)
    print(f"\né…ç½®:")
    print(f"  åŸå§‹æ¨¡å‹: {MODEL_PATH}")
    print(f"  éªŒè¯é›†: {VAL_DIR}")
    print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  è®¾å¤‡: {DEVICE}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ========== Step 1: åŠ è½½æ¨¡å‹ ==========
    print("\n" + "="*60)
    print("Step 1: åŠ è½½åŸå§‹æ¨¡å‹")
    print("="*60)
    
    from efficientnet_pytorch import EfficientNet
    
    model = EfficientNet.from_name('efficientnet-b0', num_classes=2)
    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True)
    model.load_state_dict(checkpoint['state_dict'])
    model = model.to(DEVICE)
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ========== Step 2: åŠ è½½éªŒè¯é›† ==========
    print("\n" + "="*60)
    print("Step 2: åŠ è½½éªŒè¯é›†")
    print("="*60)
    
    from torchvision import transforms, datasets
    import PIL
    
    image_size = 224
    normalize = transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1, 1, 1])
    
    val_transforms = transforms.Compose([
        transforms.Resize(image_size, interpolation=PIL.Image.BICUBIC),
        transforms.CenterCrop(image_size),
        transforms.ToTensor(),
        normalize,
    ])
    
    val_dataset = datasets.ImageFolder(VAL_DIR, val_transforms)
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=64,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    print(f"âœ… éªŒè¯é›†åŠ è½½æˆåŠŸ (æ ·æœ¬æ•°: {len(val_dataset)})")
    
    # ========== Step 3: é˜ˆå€¼ä¼˜åŒ– ==========
    print("\n" + "="*60)
    print("Step 3: é˜ˆå€¼ä¼˜åŒ– (å¯»æ‰¾æœ€ä¼˜åˆ†ç±»é˜ˆå€¼)")
    print("="*60)
    
    from precision_recall_optimizer import ThresholdOptimizer
    
    optimal_threshold, threshold_metrics = ThresholdOptimizer.find_optimal_threshold_constrained(
        model=model,
        val_loader=val_loader,
        device=DEVICE,
        min_recall=MIN_RECALL,
        plot=True
    )
    
    # ä¿å­˜é˜ˆå€¼ç»“æœ
    threshold_result = {
        'optimal_threshold': float(optimal_threshold),
        'metrics': {k: float(v) for k, v in threshold_metrics.items()}
    }
    torch.save(threshold_result, os.path.join(OUTPUT_DIR, 'threshold_optimization.pt'))
    
    print(f"\nâœ… é˜ˆå€¼ä¼˜åŒ–å®Œæˆ")
    print(f"   æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    print(f"   å¬å›ç‡: {threshold_metrics['recall']*100:.2f}%")
    print(f"   ç²¾ç¡®ç‡: {threshold_metrics['precision']*100:.2f}%")
    print(f"   F1åˆ†æ•°: {threshold_metrics['f1']*100:.2f}%")
    
    # ========== Step 4: ç”Ÿæˆä¼˜åŒ–æ¨¡å‹ (.pthæƒé‡) ==========
    print("\n" + "="*60)
    print("Step 4: ç”Ÿæˆä¼˜åŒ–æ¨¡å‹æƒé‡")
    print("="*60)
    
    from inference_optimization import optimize_model_for_deployment
    
    optimized_models, save_paths = optimize_model_for_deployment(
        model_path=MODEL_PATH,
        val_loader=val_loader,
        device=DEVICE,
        output_dir=OUTPUT_DIR
    )
    
    # ========== Step 5: æ€§èƒ½æµ‹è¯• ==========
    print("\n" + "="*60)
    print("Step 5: æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60)
    
    from inference_optimization import benchmark_models
    
    test_models = {
        'åŸå§‹æ¨¡å‹': model,
        **optimized_models
    }
    
    speed_results = benchmark_models(
        models_dict=test_models,
        val_loader=val_loader,
        device=DEVICE,
        num_batches=50
    )
    
    # æ‰“å°å¯¹æ¯”
    print("\n" + "="*60)
    print("æ€§èƒ½å¯¹æ¯”")
    print("="*60)
    print(f"{'æ¨¡å‹ç±»å‹':<15} {'æ¨ç†æ—¶é—´(ms)':<15} {'å‡†ç¡®ç‡':<10} {'åŠ é€Ÿæ¯”':<10}")
    print("-" * 60)
    
    for name, metrics in speed_results.items():
        speedup = metrics.get('speedup', '-')
        print(f"{name:<15} {metrics['avg_inference_time_ms']:<15.2f} "
              f"{metrics['accuracy']:<10.4f} {speedup:<10}")
    
    # ========== Step 6: ç”Ÿæˆéƒ¨ç½²é…ç½® ==========
    print("\n" + "="*60)
    print("Step 6: ç”Ÿæˆéƒ¨ç½²é…ç½®")
    print("="*60)
    
    # é€‰æ‹©æœ€å¿«çš„æ¨¡å‹
    optimized_results = {k: v for k, v in speed_results.items() if k != 'åŸå§‹æ¨¡å‹'}
    if optimized_results:
        best_model_name = min(optimized_results, key=lambda x: optimized_results[x]['avg_inference_time_ms'])
        best_model_path = save_paths.get(best_model_name, '')
    else:
        best_model_name = 'åŸå§‹æ¨¡å‹'
        best_model_path = MODEL_PATH
    
    deployment_config = {
        'model_type': best_model_name,
        'model_path': best_model_path,
        'threshold': float(optimal_threshold),
        'use_fp16': best_model_name == 'fp16',
        'device': DEVICE,
        'target_recall': MIN_RECALL,
        'achieved_metrics': {
            'recall': float(threshold_metrics['recall']),
            'precision': float(threshold_metrics['precision']),
            'f1': float(threshold_metrics['f1'])
        },
        'inference_time_ms': float(speed_results[best_model_name]['avg_inference_time_ms'])
    }
    
    import json
    config_path = os.path.join(OUTPUT_DIR, 'deployment_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(deployment_config, f, indent=4, ensure_ascii=False)
    
    print(f"\nâœ… éƒ¨ç½²é…ç½®å·²ä¿å­˜: {config_path}")
    print("\né…ç½®å†…å®¹:")
    print(json.dumps(deployment_config, indent=4, ensure_ascii=False))
    
    # ========== æœ€ç»ˆæ€»ç»“ ==========
    print("\n" + "="*80)
    print("ä¼˜åŒ–å®Œæˆæ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ“Š æ€§èƒ½æå‡:")
    original_metrics = speed_results['åŸå§‹æ¨¡å‹']
    best_metrics = speed_results[best_model_name]
    
    print(f"  æ¨ç†é€Ÿåº¦: {original_metrics['avg_inference_time_ms']:.2f}ms â†’ {best_metrics['avg_inference_time_ms']:.2f}ms")
    print(f"  åŠ é€Ÿæ¯”: {best_metrics.get('speedup', 'N/A')}")
    print(f"  å‡†ç¡®ç‡: {best_metrics['accuracy']*100:.2f}%")
    
    print(f"\nğŸ“Š åˆ†ç±»æ€§èƒ½:")
    print(f"  å¬å›ç‡: {threshold_metrics['recall']*100:.2f}%")
    print(f"  ç²¾ç¡®ç‡: {threshold_metrics['precision']*100:.2f}%")
    print(f"  F1åˆ†æ•°: {threshold_metrics['f1']*100:.2f}%")
    print(f"  æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    
    print(f"\nğŸ“¦ è¾“å‡ºæ–‡ä»¶:")
    print(f"  {OUTPUT_DIR}/")
    print(f"    â”œâ”€â”€ model_optimized.pth          # åŸºç¡€ä¼˜åŒ–æ¨¡å‹")
    if DEVICE == 'cuda':
        print(f"    â”œâ”€â”€ model_fp16.pth               # FP16ä¼˜åŒ–æ¨¡å‹ (æ¨è)")
    print(f"    â”œâ”€â”€ threshold_optimization.pt    # é˜ˆå€¼ä¼˜åŒ–ç»“æœ")
    print(f"    â”œâ”€â”€ threshold_optimization.png   # é˜ˆå€¼æ›²çº¿å›¾")
    print(f"    â””â”€â”€ deployment_config.json       # éƒ¨ç½²é…ç½®")
    
    print("\nâœ… æ‰€æœ‰ä¼˜åŒ–å®Œæˆï¼")
    
    # ========== ä½¿ç”¨è¯´æ˜ ==========
    print("\n" + "="*80)
    print("å¿«é€Ÿä½¿ç”¨æŒ‡å—")
    print("="*80)
    
    print("\næ–¹æ³•1: ä½¿ç”¨FastInferenceç±» (æ¨è)")
    print("-" * 60)
    print(f"""
from inference_optimization import FastInference

# åŠ è½½ä¼˜åŒ–æ¨¡å‹
model = FastInference(
    model_path='{best_model_path}',
    device='{DEVICE}',
    use_fp16={best_model_name == 'fp16'}
)

# æ¨ç†
is_overflow, prob = model.predict(image_tensor, threshold={optimal_threshold:.4f})
print(f"æº¢å‡º: {{is_overflow}}, æ¦‚ç‡: {{prob:.4f}}")
""")
    
    print("\næ–¹æ³•2: ä¼ ç»Ÿæ–¹å¼ (å…¼å®¹æ€§æœ€å¥½)")
    print("-" * 60)
    print(f"""
from efficientnet_pytorch import EfficientNet
import torch

model = EfficientNet.from_name('efficientnet-b0', num_classes=2)
model.load_state_dict(torch.load('{best_model_path}'))
model = model.{DEVICE}().eval()
{'model = model.half()  # FP16æ¨¡å¼' if best_model_name == 'fp16' else ''}

with torch.no_grad():
    output = model(image)
    prob = torch.softmax(output, dim=1)[0, 1].item()
    is_overflow = prob > {optimal_threshold:.4f}
""")


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
